{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding = utf-8\n",
    "#中文分词，保存分词结果。\n",
    "import jieba\n",
    "\n",
    "fnInput = r\"C:\\Users\\李梓童\\Datas\\带标签短信.txt\"\n",
    "fnOutput = r\"C:\\Users\\李梓童\\Datas\\带标签短信_seg.txt\"\n",
    "fin = open(fnInput,'r' , encoding = 'UTF-8' )\n",
    "fout = open(r\"C:\\Users\\李梓童\\Datas\\带标签短信_seg.txt\", \"a+\", encoding = 'UTF-8')\n",
    "numline = 0\n",
    "\n",
    "print('start\\n')\n",
    "\n",
    "for line in fin:\n",
    "    seg_list = jieba.cut(line, cut_all=False)\n",
    "    fout.write(' '.join(seg_list))\n",
    "    numline += 1\n",
    "\n",
    "fin.close()\n",
    "fout.close()\n",
    "\n",
    "print(\"processed %d lines\",numline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Precision = 0.6327, Recall = 1.0000, F-Score = 0.7750, Accuracy = 0.9268\n",
      "0.0001, 0.9999\n",
      "0.0069, 0.9931\n",
      "0.0001, 0.9999\n",
      "0.0000, 1.0000\n",
      "0.0001, 0.9999\n",
      "0.0014, 0.9986\n",
      "0.0000, 1.0000\n",
      "1.0000, 0.0000\n",
      "0.0018, 0.9982\n",
      "0.0001, 0.9999\n",
      "0.0120, 0.9880\n",
      "0.0792, 0.9208\n",
      "0.8176, 0.1824\n",
      "0.4652, 0.5348\n",
      "0.0007, 0.9993\n",
      "0.0001, 0.9999\n",
      "0.0001, 0.9999\n",
      "0.0006, 0.9994\n",
      "0.0016, 0.9984\n",
      "0.0005, 0.9995\n",
      "0.0010, 0.9990\n",
      "0.9729, 0.0271\n",
      "0.0015, 0.9985\n",
      "0.0001, 0.9999\n",
      "0.0053, 0.9947\n",
      "0.0132, 0.9868\n",
      "0.0001, 0.9999\n",
      "0.0036, 0.9964\n",
      "0.0002, 0.9998\n",
      "0.0022, 0.9978\n",
      "0.0001, 0.9999\n",
      "0.0000, 1.0000\n",
      "0.0004, 0.9996\n",
      "0.0085, 0.9915\n",
      "0.0118, 0.9882\n",
      "1.0000, 0.0000\n",
      "0.0002, 0.9998\n",
      "0.9973, 0.0027\n",
      "0.0070, 0.9930\n",
      "0.0096, 0.9904\n",
      "1.0000, 0.0000\n",
      "0.0022, 0.9978\n",
      "0.0023, 0.9977\n",
      "1.0000, 0.0000\n",
      "0.0010, 0.9990\n",
      "0.0003, 0.9997\n",
      "0.0007, 0.9993\n",
      "0.0005, 0.9995\n",
      "0.9397, 0.0603\n",
      "0.8708, 0.1292\n",
      "0.0001, 0.9999\n",
      "0.0000, 1.0000\n",
      "0.0000, 1.0000\n",
      "0.0025, 0.9975\n",
      "0.0058, 0.9942\n",
      "1.0000, 0.0000\n",
      "0.0062, 0.9938\n",
      "1.0000, 0.0000\n",
      "1.0000, 0.0000\n",
      "0.9931, 0.0069\n",
      "0.0227, 0.9773\n",
      "1.0000, 0.0000\n",
      "0.0000, 1.0000\n",
      "0.0001, 0.9999\n",
      "1.0000, 0.0000\n",
      "0.0081, 0.9919\n",
      "1.0000, 0.0000\n",
      "1.0000, 0.0000\n",
      "0.0013, 0.9987\n",
      "0.0055, 0.9945\n",
      "0.0000, 1.0000\n",
      "0.0002, 0.9998\n",
      "0.0011, 0.9989\n",
      "0.0407, 0.9593\n",
      "0.0001, 0.9999\n",
      "1.0000, 0.0000\n",
      "0.0000, 1.0000\n",
      "0.0044, 0.9956\n",
      "0.0001, 0.9999\n",
      "0.0088, 0.9912\n",
      "0.0001, 0.9999\n",
      "0.1492, 0.8508\n",
      "1.0000, 0.0000\n",
      "1.0000, 0.0000\n",
      "0.9964, 0.0036\n",
      "0.0001, 0.9999\n",
      "0.0010, 0.9990\n",
      "0.0094, 0.9906\n",
      "0.0006, 0.9994\n",
      "0.0005, 0.9995\n",
      "0.0000, 1.0000\n",
      "1.0000, 0.0000\n",
      "0.0199, 0.9801\n",
      "0.0000, 1.0000\n",
      "0.0021, 0.9979\n",
      "0.0000, 1.0000\n",
      "0.0000, 1.0000\n",
      "0.0183, 0.9817\n",
      "1.0000, 0.0000\n",
      "0.0009, 0.9991\n",
      "0.9246, 0.0754\n",
      "0.0281, 0.9719\n",
      "0.0083, 0.9917\n",
      "0.0000, 1.0000\n",
      "0.0025, 0.9975\n",
      "0.0002, 0.9998\n",
      "0.2521, 0.7479\n",
      "0.0001, 0.9999\n",
      "0.0003, 0.9997\n",
      "0.0000, 1.0000\n",
      "1.0000, 0.0000\n",
      "0.0013, 0.9987\n",
      "0.0088, 0.9912\n",
      "1.0000, 0.0000\n",
      "0.0000, 1.0000\n",
      "0.0002, 0.9998\n",
      "1.0000, 0.0000\n",
      "0.0003, 0.9997\n",
      "1.0000, 0.0000\n",
      "0.0108, 0.9892\n",
      "0.0001, 0.9999\n",
      "0.0302, 0.9698\n",
      "1.0000, 0.0000\n",
      "0.7542, 0.2458\n",
      "0.0000, 1.0000\n",
      "0.0014, 0.9986\n",
      "0.0014, 0.9986\n",
      "0.0001, 0.9999\n",
      "0.0898, 0.9102\n",
      "0.0000, 1.0000\n",
      "0.0029, 0.9971\n",
      "0.0004, 0.9996\n",
      "0.0010, 0.9990\n",
      "0.0010, 0.9990\n",
      "0.0006, 0.9994\n",
      "0.4561, 0.5439\n",
      "0.0021, 0.9979\n",
      "0.0095, 0.9905\n",
      "0.6919, 0.3081\n",
      "0.0058, 0.9942\n",
      "0.0628, 0.9372\n",
      "0.0001, 0.9999\n",
      "0.0217, 0.9783\n",
      "0.8201, 0.1799\n",
      "1.0000, 0.0000\n",
      "0.0000, 1.0000\n",
      "0.7048, 0.2952\n",
      "0.0001, 0.9999\n",
      "0.0003, 0.9997\n",
      "0.0047, 0.9953\n",
      "0.0032, 0.9968\n",
      "0.0030, 0.9970\n",
      "0.0001, 0.9999\n",
      "0.9233, 0.0767\n",
      "0.0001, 0.9999\n",
      "0.0007, 0.9993\n",
      "0.0001, 0.9999\n",
      "0.0009, 0.9991\n",
      "0.2726, 0.7274\n",
      "0.0368, 0.9632\n",
      "0.0020, 0.9980\n",
      "0.2744, 0.7256\n",
      "1.0000, 0.0000\n",
      "0.0057, 0.9943\n",
      "0.0007, 0.9993\n",
      "1.0000, 0.0000\n",
      "0.0004, 0.9996\n",
      "0.0000, 1.0000\n",
      "0.0001, 0.9999\n",
      "0.0005, 0.9995\n",
      "0.0001, 0.9999\n",
      "0.0044, 0.9956\n",
      "0.0010, 0.9990\n",
      "0.0057, 0.9943\n",
      "0.0002, 0.9998\n",
      "0.0001, 0.9999\n",
      "0.5196, 0.4804\n",
      "0.0001, 0.9999\n",
      "0.0153, 0.9847\n",
      "1.0000, 0.0000\n",
      "0.5394, 0.4606\n",
      "0.0142, 0.9858\n",
      "0.0228, 0.9772\n",
      "1.0000, 0.0000\n",
      "0.0021, 0.9979\n",
      "0.0034, 0.9966\n",
      "0.0002, 0.9998\n",
      "0.0001, 0.9999\n",
      "0.0002, 0.9998\n",
      "0.0070, 0.9930\n",
      "0.0012, 0.9988\n",
      "0.0000, 1.0000\n",
      "0.9845, 0.0155\n",
      "0.0001, 0.9999\n",
      "1.0000, 0.0000\n",
      "0.0001, 0.9999\n",
      "0.0004, 0.9996\n",
      "0.0255, 0.9745\n",
      "0.0001, 0.9999\n",
      "0.0001, 0.9999\n",
      "0.0006, 0.9994\n",
      "1.0000, 0.0000\n",
      "0.0008, 0.9992\n",
      "0.0052, 0.9948\n",
      "0.0003, 0.9997\n",
      "0.0002, 0.9998\n",
      "1.0000, 0.0000\n",
      "1.0000, 0.0000\n",
      "0.0001, 0.9999\n",
      "0.0001, 0.9999\n",
      "0.0017, 0.9983\n",
      "0.0000, 1.0000\n",
      "0.0001, 0.9999\n",
      "0.0006, 0.9994\n",
      "0.0003, 0.9997\n",
      "1.0000, 0.0000\n",
      "0.0008, 0.9992\n",
      "0.0008, 0.9992\n",
      "0.0004, 0.9996\n",
      "0.0003, 0.9997\n",
      "0.0005, 0.9995\n",
      "0.0002, 0.9998\n",
      "0.0010, 0.9990\n",
      "0.0004, 0.9996\n",
      "0.0006, 0.9994\n",
      "0.0081, 0.9919\n",
      "0.0036, 0.9964\n",
      "0.0153, 0.9847\n",
      "0.2267, 0.7733\n",
      "0.0103, 0.9897\n",
      "1.0000, 0.0000\n",
      "0.0003, 0.9997\n",
      "0.0003, 0.9997\n",
      "0.0003, 0.9997\n",
      "0.0002, 0.9998\n",
      "0.0008, 0.9992\n",
      "0.0095, 0.9905\n",
      "0.0029, 0.9971\n",
      "0.0000, 1.0000\n",
      "0.0001, 0.9999\n",
      "0.9924, 0.0076\n",
      "0.0002, 0.9998\n",
      "0.0026, 0.9974\n",
      "0.0671, 0.9329\n",
      "0.0000, 1.0000\n",
      "0.0000, 1.0000\n"
     ]
    }
   ],
   "source": [
    "#encoding  = utf-8\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import os\n",
    "\n",
    "file_train = open(r'C:\\Users\\李梓童\\Datas\\带标签短信_seg2.txt','r', encoding = 'UTF-8')\n",
    "file_test = open(r'C:\\Users\\李梓童\\Datas\\带标签短信_seg3.txt','r', encoding = 'UTF-8')\n",
    "tmp_pair=()\n",
    "training_set = []\n",
    "\n",
    "for line in file_train:\n",
    "    val = ord(line[0])-48\n",
    "\n",
    "#    if(val > 2):\n",
    "#        val = 0\n",
    "    \n",
    "    tmp_str = line[4:-1]\n",
    "    tmp_pair = (tmp_str,val)\n",
    "    training_set.append(tmp_pair)\n",
    "\n",
    "#print(training_set)\n",
    "\n",
    "def voc(data):\n",
    "    voc = {}\n",
    "    for (sentence,val) in data:\n",
    "        words = sentence.lower().split()\n",
    "        for w in words:\n",
    "            voc[w]=1\n",
    "    return voc\n",
    "\n",
    "def feature(data,v):\n",
    "    ftr = []\n",
    "    for (sentence,label) in data:\n",
    "        f = dict((w,0) for w in v.keys() )\n",
    "        words = sentence.lower().split()\n",
    "        for w in words:\n",
    "            f[w]=1 #该句中出现过的特征向量中的词赋值为1\n",
    "        ftr.append((f,label))\n",
    "    return ftr\n",
    "\n",
    "# 特征构建：选出所有出现过的词\n",
    "v = voc(training_set)\n",
    "#print(v)\n",
    "\n",
    "# 特征抽取：若在某个句子中，某个词出现过，则该词标记为1，\n",
    "# 否则标记为0，这样的特征向量与该句的标签（0或1）构成特征空间\n",
    "train_feature_label = feature(training_set,v)\n",
    "#print(train_feature_label)\n",
    "classifier = NaiveBayesClassifier.train(train_feature_label)\n",
    "print('done')\n",
    "\n",
    "file_test = open(r'C:\\Users\\李梓童\\Datas\\带标签短信_seg3.txt','r', encoding = 'UTF-8')\n",
    "tmp_pair_2=()\n",
    "test_set = []\n",
    "\n",
    "for line in file_test:\n",
    "    val = ord(line[0])-48\n",
    "    \n",
    "    # 修复在第一行出现的bug\n",
    "    if(val > 2):\n",
    "        val = 0\n",
    "    \n",
    "    tmp_str = line[4:-1]\n",
    "    tmp_pair_2 = (tmp_str,val)\n",
    "    test_set.append(tmp_pair_2)\n",
    "\n",
    "test_feature = []\n",
    "test_labels = []\n",
    "\n",
    "for (ftr,label) in feature(test_set,v):\n",
    "    test_feature.append(ftr)\n",
    "    test_labels.append(label)\n",
    "\n",
    "pred_labels = classifier.classify_many(test_feature)\n",
    "print(test_labels)\n",
    "print(pred_labels)\n",
    "\n",
    "def classify_eval(truth,pred):\n",
    "    idx = 0\n",
    "    (TP, FP, TN, FN) = (0, 0, 0, 0)\n",
    "    for truth_label in truth:\n",
    "        pred_label = pred[idx]\n",
    "        if( truth_label == 1 and pred_label == 1 ):\n",
    "            TP = TP + 1\n",
    "        elif( truth_label == 0 and pred_label == 0 ):\n",
    "            TN = TN +1\n",
    "        elif( truth_label == 1 and pred_label == 0 ):\n",
    "            FN = FN + 1\n",
    "        elif( truth_label == 0 and pred_label == 1 ):\n",
    "            FP = FP + 1\n",
    "        idx = idx + 1\n",
    "    P = 0 if TP == 0 else TP / (TP + FP)\n",
    "    R = 0 if TP == 0 else TP / (TP + FN)\n",
    "    F = 0 if (P == 0 or R == 0) else 2* P *R/(P + R)\n",
    "    Acc = 0 if (TP + TN == 0) else (TP + TN)/(TP + TN + FP + FN)\n",
    "    return (P,R,F,Acc)\n",
    "\n",
    "print('Precision = %.4f, Recall = %.4f, F-Score = %.4f, Accuracy = %.4f'%classify_eval(test_labels,pred_labels))\n",
    "\n",
    "probs = classifier.prob_classify_many(test_feature)\n",
    "for pdist in probs:\n",
    "    print('%.4f, %.4f'%(pdist.prob(1),pdist.prob(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding  = utf-8\n",
    "#判断所有未标签短信\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "import os\n",
    "\n",
    "#文件\n",
    "file_train = r'C:\\Users\\李梓童\\Datas\\带标签短信_seg.txt'\n",
    "file_test = r'C:\\Users\\李梓童\\Datas\\不带标签短信_seg.txt'\n",
    "file_test_nonseg = r'C:\\Users\\李梓童\\Datas\\不带标签短信.txt'\n",
    "file_result = r'C:\\Users\\李梓童\\Datas\\短信分类结果.txt'\n",
    "file_sw = r'C:\\Users\\李梓童\\Datas\\chineseStopWords.txt'\n",
    "#全局变量\n",
    "StopWords = []\n",
    "Testdata_nonseg = []\n",
    "Testdata_seg = []\n",
    "\n",
    "print('start')\n",
    "\n",
    "#输出：形如（已分好词的含空格字符串，判断值）组成的集合\n",
    "def Generate_Set(filename):\n",
    "    tmp_pair=()\n",
    "    tmp_set = []\n",
    "    \n",
    "    file_read = open ( filename , 'r', encoding = 'UTF-8' )\n",
    "    for line in file_read:\n",
    "        val = ord(line[0])-48\n",
    "\n",
    "#        if(val > 2):\n",
    "#            val = 0\n",
    "    \n",
    "        tmp_str = line[4:-1]\n",
    "        tmp_pair = (tmp_str,val)\n",
    "        tmp_set.append(tmp_pair)\n",
    "    \n",
    "    file_read.close()\n",
    "    print('Generate_Set:\"'+ filename + '\":done')\n",
    "    return tmp_set\n",
    "\n",
    "#生成停止词词表\n",
    "def GetStopWords():\n",
    "    file_read = open ( file_sw , 'r', encoding = 'UTF-8' )\n",
    "    for line in file_read:\n",
    "        StopWords.append(line[:-1])\n",
    "    file_read.close()\n",
    "    print('GetStopWords:done')\n",
    "\n",
    "#生成测试短信表（无空格版本），用于写入result中    \n",
    "def GetTestNonseg():\n",
    "    file_read = open ( file_test_nonseg , 'r', encoding = 'UTF-8' )\n",
    "    for line in file_read:\n",
    "        Testdata_nonseg.append(line[:-1])\n",
    "    file_read.close()\n",
    "    print(Testdata_nonseg)\n",
    "    print('GetTestNonseg:done')    \n",
    "\n",
    "#生成测试词表\n",
    "def GetTestSet():\n",
    "    file_read = open ( file_test , 'r', encoding = 'UTF-8' )\n",
    "    for line in file_read:\n",
    "        Testdata_seg.append(line[:-1])\n",
    "    \n",
    "    print(Testdata_seg)\n",
    "    file_read.close()\n",
    "    print('GetTestSet:done')\n",
    "    \n",
    "#初始化\n",
    "GetStopWords()\n",
    "GetTestNonseg()\n",
    "GetTestSet()\n",
    "\n",
    "#承接上面的Generate_Set，输出向量\n",
    "def voc(data):\n",
    "    voc = {}\n",
    "    for (sentence,val) in data:\n",
    "        words = sentence.lower().split()\n",
    "        for w in words:\n",
    "            if( w not in StopWords): #去掉停止词\n",
    "                voc[w]=1\n",
    "    print('voc:done')\n",
    "    return voc\n",
    "\n",
    "def feature(data,v):\n",
    "    ftr = []\n",
    "    for (sentence,label) in data:\n",
    "        f = dict((w,0) for w in v.keys() )\n",
    "        words = sentence.lower().split()\n",
    "        for w in words:\n",
    "            if ( w not in StopWords ): #去掉停止词\n",
    "                f[w]=1 #该句中出现过的特征向量中的词赋值为1\n",
    "        ftr.append((f,label))\n",
    "    return ftr\n",
    "\n",
    "def feature_for_test(data,v):\n",
    "    ftr = []\n",
    "    for sentence in data:\n",
    "        f = dict((w,0) for w in v.keys())\n",
    "        words = sentence.lower().split()\n",
    "        for w in words:\n",
    "            if ( w not in StopWords ): #去掉停止词\n",
    "                f[w]=1 #该句中出现过的特征向量中的词赋值为1\n",
    "        ftr.append(f)\n",
    "    return ftr\n",
    "\n",
    "def WriteResult(pred_labels,test_set):\n",
    "    file_write = open(file_result,'a+', encoding = 'UTF-8')\n",
    "    space = ' \\t '\n",
    "    for i in range(len(pred_labels)):\n",
    "        tmp_content=str(test_set[i])\n",
    "        tmp_label=str(pred_labels[i])\n",
    "        file_write.write(tmp_label + space + tmp_content + '\\n')\n",
    "\n",
    "training_set = Generate_Set(file_train)\n",
    "\n",
    "# 特征构建：选出所有出现过的词\n",
    "v = voc(training_set)\n",
    "\n",
    "# 特征抽取：若在某个句子中，某个词出现过，则该词标记为1，\n",
    "# 否则标记为0，这样的特征向量与该句的标签（0或1）构成特征空间\n",
    "train_feature_label = feature(training_set,v)\n",
    "classifier = NaiveBayesClassifier.train(train_feature_label)\n",
    "print('traindata_bayes:done')\n",
    "\n",
    "test_feature = []\n",
    "\n",
    "print(Testdata_nonseg)\n",
    "for ftr in feature_for_test(Testdata_seg,v):\n",
    "    test_feature.append(ftr)\n",
    "\n",
    "pred_labels = classifier.classify_many(test_feature)\n",
    "print(pred_labels)\n",
    "\n",
    "WriteResult(pred_labels, Testdata_nonseg)\n",
    "\n",
    "print('All:done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "#用于生成可以导入kibana的json文件\n",
    "\n",
    "a = open(r'C:\\Users\\李梓童\\Datas\\短信分类结果.txt', \"r\",encoding='utf-8')\n",
    "out = a.read()\n",
    "TypeList = out.split('\\n')\n",
    "#print(TypeList)\n",
    "\n",
    "lenth = len(TypeList)\n",
    "print(lenth)\n",
    "\n",
    "number = 1\n",
    "ju_1 = '{\"index\":{\"_index\":\"text\",\"_id\":'\n",
    "ju_2 = '{\"text_entry\":\"'\n",
    "ju_3 = ' \"value\":\"'\n",
    "\n",
    "# print(ju_1)\n",
    "for x in TypeList:\n",
    "\n",
    "    res_1 = ju_1 + str(number) + '}}'+'\\n'\n",
    "    print(res_1)\n",
    "    a = open(r\"C:\\Users\\李梓童\\Datas\\out.json\", \"a\", encoding='UTF-8')\n",
    "    a.write(res_1)\n",
    "\n",
    "    val = x[0]\n",
    "    tmp_str = x[4:]\n",
    "    res_2 = ju_2 + tmp_str + '\",' + ju_3 + val + '\"}'+'\\n'\n",
    "    print(res_2)\n",
    "    a = open(r\"C:\\Users\\李梓童\\Datas\\out.json\", \"a\", encoding='UTF-8')\n",
    "    a.write(res_2)\n",
    "\n",
    "\n",
    "    a.close()\n",
    "    number+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
